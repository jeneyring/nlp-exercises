{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "824d76aa",
   "metadata": {},
   "source": [
    "### Prepare & Parsing text/data:\n",
    "After acquiring the data with NLP/web scraping methods, we need to parse the data into small bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a70b0e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jeneyring/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jeneyring/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/jeneyring/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#need to import nltk (natural language tool kit) to help with parsing:\n",
    "import nltk; nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205bf271",
   "metadata": {},
   "source": [
    "Steps to parsing data:\n",
    "<br>\n",
    "    1) Convert text to all lower case for normalcy.<br>\n",
    "    2) Remove any accented characters, non-ASCII characters.<br>\n",
    "    3) Remove special characters.<br>\n",
    "    4) Stem or lemmatize the words.(stem = \"if b, then c\")<br>\n",
    "    5) Remove stopwords.(if, and, the, etc)<br>\n",
    "    6) Store the clean text and the original text for use in future notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05acc43",
   "metadata": {},
   "source": [
    "#NOTE: if your corpus is large enough, its ok to really clean up/strip down your text to become natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9056b38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from time import strftime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import acquire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a38d364",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "original = acquire.get_news_articles()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa096cc",
   "metadata": {},
   "source": [
    "### Exercise 1)\n",
    "Define a function named basic_clean. It should take in a string and apply some basic text cleaning to it:\n",
    "\n",
    "Lowercase everything\n",
    "Normalize unicode characters\n",
    "Replace anything that is not a letter, number, whitespace or a single quote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c6067b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>source</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You all call me fugitive, which court has ever...</td>\n",
       "      <td>Lalit Modi on Sunday took to Instagram to spea...</td>\n",
       "      <td>Ridham Gambhir</td>\n",
       "      <td>17 Jul</td>\n",
       "      <td>NaN</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>World's biggest NFT marketplace OpenSea fires ...</td>\n",
       "      <td>The world's first and biggest NFT marketplace ...</td>\n",
       "      <td>Ridham Gambhir</td>\n",
       "      <td>16 Jul</td>\n",
       "      <td>NaN</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BCCI had ₹40 cr in bank when I joined &amp; ₹47,68...</td>\n",
       "      <td>In an Instagram post, Lalit Modi asserted that...</td>\n",
       "      <td>Ridham Gambhir</td>\n",
       "      <td>17 Jul</td>\n",
       "      <td>NaN</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A fighter to the core: Mahindra praises PV Sin...</td>\n",
       "      <td>Businessman Anand Mahindra took to Twitter to ...</td>\n",
       "      <td>Ridham Gambhir</td>\n",
       "      <td>17 Jul</td>\n",
       "      <td>NaN</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Twitter's sudden speed for trial after 2 month...</td>\n",
       "      <td>Tesla CEO Elon Musk has opposed Twitter's requ...</td>\n",
       "      <td>Ridham Gambhir</td>\n",
       "      <td>16 Jul</td>\n",
       "      <td>NaN</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  You all call me fugitive, which court has ever...   \n",
       "1  World's biggest NFT marketplace OpenSea fires ...   \n",
       "2  BCCI had ₹40 cr in bank when I joined & ₹47,68...   \n",
       "3  A fighter to the core: Mahindra praises PV Sin...   \n",
       "4  Twitter's sudden speed for trial after 2 month...   \n",
       "\n",
       "                                             content          author    date  \\\n",
       "0  Lalit Modi on Sunday took to Instagram to spea...  Ridham Gambhir  17 Jul   \n",
       "1  The world's first and biggest NFT marketplace ...  Ridham Gambhir  16 Jul   \n",
       "2  In an Instagram post, Lalit Modi asserted that...  Ridham Gambhir  17 Jul   \n",
       "3  Businessman Anand Mahindra took to Twitter to ...  Ridham Gambhir  17 Jul   \n",
       "4  Tesla CEO Elon Musk has opposed Twitter's requ...  Ridham Gambhir  16 Jul   \n",
       "\n",
       "   source  category  \n",
       "0     NaN  business  \n",
       "1     NaN  business  \n",
       "2     NaN  business  \n",
       "3     NaN  business  \n",
       "4     NaN  business  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#look at the df to consider what does need to be applied: \n",
    "#ie. I want to keep my titles upper case but my content can be lowercase\n",
    "original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42a18b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lalit Modi on Sunday took to Instagram to speak about various issues after he revealed his relationship with Sushmita Sen. He wrote, \"[Though you all] call me a \"fugitive\"...tell me which court has ever convicted me...None…Everyone knows how difficult it is to do business in India.\" Speaking about IPL, he said, “Everyone...knows that I did it all alone\"'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#applying lowercase to one string in dataframe:\n",
    "string = original.content[0]\n",
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00e85315",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lowering all capitalized letters:\n",
    "string = string.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1697f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing the content of string:\n",
    "string=unicodedata.normalize('NFKD', string).encode('ascii', 'ignore').decode('utf-8', 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61bcd864",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing anything not a letter, number, whitespace or single quote:\n",
    "string = re.sub(r\"[^a-z0-9'\\s]\", '' ,string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a729d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lalit modi on sunday took to instagram to speak about various issues after he revealed his relationship with sushmita sen he wrote though you all call me a fugitivetell me which court has ever convicted menoneeveryone knows how difficult it is to do business in india speaking about ipl he said everyoneknows that i did it all alone'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking output:\n",
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "147a37c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#putting altogether in a function:\n",
    "\n",
    "def basic_clean(string):\n",
    "    \"\"\"A function that uses NLTK to clean and normalizes a string\"\"\"\n",
    "    string = string.lower()\n",
    "    string = unicodedata.normalize('NFKD', string).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    string = re.sub(r\"[^a-z0-9'\\s]\", '' ,string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97430ac",
   "metadata": {},
   "source": [
    "Another way to do this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5080e9fe",
   "metadata": {},
   "source": [
    "### Exercise 2)\n",
    "Define a function named tokenize. It should take in a string and tokenize all the words in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a8c211d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lalit modi on sunday took to instagram to speak about various issues after he revealed his relationship with sushmita sen he wrote though you all call me a fugitivetell me which court has ever convicted menoneeveryone knows how difficult it is to do business in india speaking about ipl he said everyoneknows that i did it all alone\n"
     ]
    }
   ],
   "source": [
    "#using toktoktokenizer to breakdown words into units:\n",
    "tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "\n",
    "print(tokenizer.tokenize(string, return_str=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7587e07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    \"\"\"This function will take in a string, tokenize by breaking any leftover words into units and return \n",
    "    the tokenized string\"\"\"\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    \n",
    "    string = tokenizer.tokenize(string, return_str=True)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cbe3664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The world\\'s first and biggest NFT marketplace OpenSea has fired 20% of its employees citing \"cryptocurrency winter\" and \"broad macroeconomic instability\". CEO Devin Finzer shared the news on Twitter and said the affected employees will be provided with a generous severance and healthcare coverage into 2023. He added that the company will also help in the placement of these employees. '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing out functions:\n",
    "original2 = acquire.get_news_articles()\n",
    "string2 = original2.content[1]\n",
    "string2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "925a460a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the world's first and biggest nft marketplace opensea has fired 20 of its employees citing cryptocurrency winter and broad macroeconomic instability ceo devin finzer shared the news on twitter and said the affected employees will be provided with a generous severance and healthcare coverage into 2023 he added that the company will also help in the placement of these employees \""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing all functions:\n",
    "cleaned = basic_clean(string2)\n",
    "cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cca3580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the world ' s first and biggest nft marketplace opensea has fired 20 of its employees citing cryptocurrency winter and broad macroeconomic instability ceo devin finzer shared the news on twitter and said the affected employees will be provided with a generous severance and healthcare coverage into 2023 he added that the company will also help in the placement of these employees\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing all functions:\n",
    "tokenize(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bee35c",
   "metadata": {},
   "source": [
    "### Exercise 3)\n",
    "Define a function named stem. It should accept some text and return the text after applying stemming to all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1998e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('call', 'call', 'call')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#defining the stem tools: stem focuses on pull the root word out of any words with affixes (pre/suffix)\n",
    "# Create the nltk stemmer object, then use it-\n",
    "ps = nltk.porter.PorterStemmer()\n",
    "\n",
    "ps.stem('call'), ps.stem('calling'), ps.stem('called')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c470b580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the world' first and biggest nft marketplac opensea ha fire 20 of it employe cite cryptocurr winter and broad macroeconom instabl ceo devin finzer share the news on twitter and said the affect employe will be provid with a gener sever and healthcar coverag into 2023 he ad that the compani will also help in the placement of these employe\n"
     ]
    }
   ],
   "source": [
    "stems = [ps.stem(word) for word in cleaned.split()]\n",
    "article_stemmed = ' '.join(stems)\n",
    "print(article_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "423063ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(string):\n",
    "    \"\"\"This function takes in a string and returns the stemmed version of string\"\"\"\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    stems = [ps.stem(word) for word in cleaned.split()]\n",
    "    string = ' '.join(stems)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d91969c",
   "metadata": {},
   "source": [
    "### Exercise 4)\n",
    "Define a function named lemmatize. It should accept some text and return the text after applying lemmatization to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e1cf0f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stem: he -- lemma: He\n",
      "stem: wa -- lemma: wa\n",
      "stem: run -- lemma: running\n",
      "stem: and -- lemma: and\n",
      "stem: eat -- lemma: eating\n",
      "stem: at -- lemma: at\n",
      "stem: same -- lemma: same\n",
      "stem: time. -- lemma: time.\n",
      "stem: he -- lemma: He\n",
      "stem: ha -- lemma: ha\n",
      "stem: bad -- lemma: bad\n",
      "stem: habit -- lemma: habit\n",
      "stem: of -- lemma: of\n",
      "stem: swim -- lemma: swimming\n",
      "stem: after -- lemma: after\n",
      "stem: play -- lemma: playing\n",
      "stem: long -- lemma: long\n",
      "stem: hour -- lemma: hour\n",
      "stem: in -- lemma: in\n",
      "stem: the -- lemma: the\n",
      "stem: sun. -- lemma: Sun.\n"
     ]
    }
   ],
   "source": [
    "#testing out lemmatizer:\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "\n",
    "for word in sentence.split():\n",
    "    print('stem:', ps.stem(word), '-- lemma:', wnl.lemmatize(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ae9ee04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stem: the -- lemma: the\n",
      "stem: world' -- lemma: world's\n",
      "stem: first -- lemma: first\n",
      "stem: and -- lemma: and\n",
      "stem: biggest -- lemma: biggest\n",
      "stem: nft -- lemma: nft\n",
      "stem: marketplac -- lemma: marketplace\n",
      "stem: opensea -- lemma: opensea\n",
      "stem: ha -- lemma: ha\n",
      "stem: fire -- lemma: fired\n",
      "stem: 20 -- lemma: 20\n",
      "stem: of -- lemma: of\n",
      "stem: it -- lemma: it\n",
      "stem: employe -- lemma: employee\n",
      "stem: cite -- lemma: citing\n",
      "stem: cryptocurr -- lemma: cryptocurrency\n",
      "stem: winter -- lemma: winter\n",
      "stem: and -- lemma: and\n",
      "stem: broad -- lemma: broad\n",
      "stem: macroeconom -- lemma: macroeconomic\n",
      "stem: instabl -- lemma: instability\n",
      "stem: ceo -- lemma: ceo\n",
      "stem: devin -- lemma: devin\n",
      "stem: finzer -- lemma: finzer\n",
      "stem: share -- lemma: shared\n",
      "stem: the -- lemma: the\n",
      "stem: news -- lemma: news\n",
      "stem: on -- lemma: on\n",
      "stem: twitter -- lemma: twitter\n",
      "stem: and -- lemma: and\n",
      "stem: said -- lemma: said\n",
      "stem: the -- lemma: the\n",
      "stem: affect -- lemma: affected\n",
      "stem: employe -- lemma: employee\n",
      "stem: will -- lemma: will\n",
      "stem: be -- lemma: be\n",
      "stem: provid -- lemma: provided\n",
      "stem: with -- lemma: with\n",
      "stem: a -- lemma: a\n",
      "stem: gener -- lemma: generous\n",
      "stem: sever -- lemma: severance\n",
      "stem: and -- lemma: and\n",
      "stem: healthcar -- lemma: healthcare\n",
      "stem: coverag -- lemma: coverage\n",
      "stem: into -- lemma: into\n",
      "stem: 2023 -- lemma: 2023\n",
      "stem: he -- lemma: he\n",
      "stem: ad -- lemma: added\n",
      "stem: that -- lemma: that\n",
      "stem: the -- lemma: the\n",
      "stem: compani -- lemma: company\n",
      "stem: will -- lemma: will\n",
      "stem: also -- lemma: also\n",
      "stem: help -- lemma: help\n",
      "stem: in -- lemma: in\n",
      "stem: the -- lemma: the\n",
      "stem: placement -- lemma: placement\n",
      "stem: of -- lemma: of\n",
      "stem: these -- lemma: these\n",
      "stem: employe -- lemma: employee\n"
     ]
    }
   ],
   "source": [
    "#using on cleaned data/shows stem vs lemmatized versions:\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "for word in cleaned.split():\n",
    "    print('stem:', ps.stem(word), '-- lemma:', wnl.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "23f846b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(string):\n",
    "    \"\"\"This function takes in a string and returns a lemmatized version of the string.\"\"\"\n",
    "    # create our lemmatizer object\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    # use a list comprehension to lemmatize each word\n",
    "    # string.split() => output a list of every token inside of the document\n",
    "    lemmas = [wnl.lemmatize(word) for word in string.split()]\n",
    "    # glue the lemmas back together by the strings we split on\n",
    "    string = ' '.join(lemmas)\n",
    "    #return the altered document\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3dd040",
   "metadata": {},
   "source": [
    "### Exercise 5)\n",
    "Define a function named remove_stopwords. It should accept some text and return the text after removing all the stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b5bbfc",
   "metadata": {},
   "source": [
    " <b>note about sets vs list: putting a list of a dictionary together. It's items cannot be replaced.</b>\n",
    "- sets have similarities to a dictionary.\n",
    "- we can use sets in stopwords so that we make sure only unique words going into list (vs. not checking what words are already in stopwords...which could return duplicates if not careful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "324ee983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#Example of list:\n",
    "list1 = [1,2,3,4]\n",
    "list2 = [2,1,3,4]\n",
    "\n",
    "print(set(list1)== set(list2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c9b8fe93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c'] {'b', 'c', 'a'}\n"
     ]
    }
   ],
   "source": [
    "#list vs set:\n",
    "mylist = ['a','b','c']\n",
    "\n",
    "myset = set(mylist)\n",
    "\n",
    "print(mylist, myset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "40120d86",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "cannot assign to function call (3073174545.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/d3/11nygz6126ndxvtrp12687680000gn/T/ipykernel_27592/3073174545.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    stopword_list = set(stopword_list)= set(exclude_words)\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m cannot assign to function call\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords(string, extra_words = [], exclude_words = []):\n",
    "    stopword_list = stopwords.words('english')\n",
    "    stopword_list = set(stopword_list)= set(exclude_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afc0ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
